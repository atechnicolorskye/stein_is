{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.distributions as ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check if multivariate normal with diagonal covariance == univariate normals with independent components \n",
    "\n",
    "class GMM(object):\n",
    "    def __init__(self, mu, sigma, weights, dim):\n",
    "        # Required parameters \n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.weights = weights\n",
    "        self.dim = dim\n",
    "        \n",
    "        distributions = []\n",
    "        for i in range(weights.shape[0]):\n",
    "            mu_, sigma_ = self.mu[i] * np.ones(dim), self.sigma[i] * np.ones(dim)\n",
    "            mvnd_i = tf.contrib.distributions.MultivariateNormalDiag(mu_, sigma_)\n",
    "            distributions.append(mvnd_i)\n",
    "        self.gauss_mix = tf.contrib.distributions.Mixture(tf.contrib.distributions.Categorical(probs=self.weights), distributions)    \n",
    "              \n",
    "#         self.x = tf.placeholder(tf.float64, [None, self.dim])\n",
    "        \n",
    "#         self.log_px()\n",
    "            \n",
    "    def log_px(self):\n",
    "        # log_px = log(sum(exp(log(w_i) + log(p_i(x)))))\n",
    "        log_px = []\n",
    "        for i in range(weights.shape[0]):\n",
    "            mu_, sigma_ = self.mu[i] * np.ones(dim), self.sigma[i] * np.ones(dim)\n",
    "            mvn = ds.MultivariateNormalDiag(loc=mu_, scale_diag=sigma_)\n",
    "            # Calculate log_px for each component\n",
    "            log_px_i = mvn.log_prob(self.x) + tf.log(weights[i])\n",
    "            if i == 0:\n",
    "                log_px = tf.reshape(log_px_i, [-1, 1])\n",
    "            else:\n",
    "                log_px = tf.concat([log_px, tf.reshape(log_px_i, [-1, 1])], axis=1)\n",
    "        self.grad = tf.gradients(tf.reduce_logsumexp(log_px, axis=1), [self.x])\n",
    "        return self.grad\n",
    "    \n",
    "    def log_px_(self, x):\n",
    "        y = tf.convert_to_tensor(x)\n",
    "        return self.gauss_mix.log_prob(y)\n",
    "       \n",
    "    def d_log_px(self, x):\n",
    "        # d_log_px = 1 / exp(log(sum(exp(log(w_i) + log(p_i(x)))))) \n",
    "        #            * sum(exp(log(w_i) + log(p_i(x)) + log(-(x - mu)/sigma^2)))\n",
    "        # Use symbolic differentiation instead\n",
    "        '''\n",
    "        # To stabilise tf.log\n",
    "        log_denominator = [] # = log_px\n",
    "        log_numerator = []\n",
    "        for i in range(weights.shape[0]):\n",
    "            mu_, sigma_ = self.mu[i] * np.ones(dim), self.sigma[i] * np.ones(dim)\n",
    "            mvn = ds.MultivariateNormalDiag(loc=mu_, scale_diag=sigma_)\n",
    "            # Calculate log_px for each component\n",
    "            log_px_i = tf.reduce_logsumexp(mvn.log_prob(self.x)) + tf.log(weights[i])\n",
    "            log_denominator.append(log_px_i)\n",
    "            # Calculate precision for each component \n",
    "            pre_i = (1./(sigma_ ** 2)) * np.eye(dim)\n",
    "            # Calculate numerator for each component\n",
    "            add_term = tf.matmul(-(self.x - mu_), pre_i)\n",
    "            log_px_i_plus_add_term = log_px_i + tf.log(add_term)\n",
    "            log_numerator.append(log_px_i_plus_add_term)\n",
    "        denominator = tf.exp(tf.reduce_logsumexp(log_denominator))\n",
    "        numerator = tf.reduce_logsumexp(log_numerator, axis=1)\n",
    "        return log_numerator\n",
    "        '''\n",
    "        log_px = self.log_px(x)\n",
    "        return tf.gradients(log_px, x)\n",
    "    \n",
    "    def d_log_px_(self, x):\n",
    "        # d_log_px = 1 / exp(log(sum(exp(log(w_i) + log(p_i(x)))))) \n",
    "        #            * sum(exp(log(w_i) + log(p_i(x)) + log(-(x - mu)/sigma^2)))\n",
    "        # Use symbolic differentiation instead\n",
    "        y = tf.convert_to_tensor(x)\n",
    "        return tf.gradients(self.gauss_mix.log_prob(y), [y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "mu = np.array([1, -1]); sigma = np.sqrt(np.array([0.1, 0.05])); weights = np.array([1./3, 2./3]); dim=6\n",
    "gmm = GMM(mu, sigma, weights, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.1 -0.1 -0.1 -0.1 -0.1 -0.1]\n",
      " [-0.1 -0.1 -0.1 -0.1 -0.1 -0.1]\n",
      " [-0.1 -0.1 -0.1 -0.1 -0.1 -0.1]\n",
      " [-0.1 -0.1 -0.1 -0.1 -0.1 -0.1]\n",
      " [-0.1 -0.1 -0.1 -0.1 -0.1 -0.1]\n",
      " [-0.1 -0.1 -0.1 -0.1 -0.1 -0.1]\n",
      " [-0.1 -0.1 -0.1 -0.1 -0.1 -0.1]\n",
      " [-0.1 -0.1 -0.1 -0.1 -0.1 -0.1]\n",
      " [-0.1 -0.1 -0.1 -0.1 -0.1 -0.1]\n",
      " [-0.1 -0.1 -0.1 -0.1 -0.1 -0.1]]\n"
     ]
    }
   ],
   "source": [
    "x = -0.1 * np.ones((10, 6)).astype(np.float64)\n",
    "print x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sess.run(gmm.grad, feed_dict={gmm.x: x})[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# x, y = sess.run([gmm.d_log_px(x), gmm.d_log_px_(x)])\n",
    "x = sess.run(gmm.d_log_px_(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 10.99788814,  10.99788814,  10.99788814,  10.99788814,\n",
      "         10.99788814,  10.99788814],\n",
      "       [ 10.99788814,  10.99788814,  10.99788814,  10.99788814,\n",
      "         10.99788814,  10.99788814],\n",
      "       [ 10.99788814,  10.99788814,  10.99788814,  10.99788814,\n",
      "         10.99788814,  10.99788814],\n",
      "       [ 10.99788814,  10.99788814,  10.99788814,  10.99788814,\n",
      "         10.99788814,  10.99788814],\n",
      "       [ 10.99788814,  10.99788814,  10.99788814,  10.99788814,\n",
      "         10.99788814,  10.99788814],\n",
      "       [ 10.99788814,  10.99788814,  10.99788814,  10.99788814,\n",
      "         10.99788814,  10.99788814],\n",
      "       [ 10.99788814,  10.99788814,  10.99788814,  10.99788814,\n",
      "         10.99788814,  10.99788814],\n",
      "       [ 10.99788814,  10.99788814,  10.99788814,  10.99788814,\n",
      "         10.99788814,  10.99788814],\n",
      "       [ 10.99788814,  10.99788814,  10.99788814,  10.99788814,\n",
      "         10.99788814,  10.99788814],\n",
      "       [ 10.99788814,  10.99788814,  10.99788814,  10.99788814,\n",
      "         10.99788814,  10.99788814]])]\n"
     ]
    }
   ],
   "source": [
    "print x\n",
    "# print y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
