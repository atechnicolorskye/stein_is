%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Ce gabarit peu servir autant les philosophes que les scientifiques ; 
% et m√™me d'autres genres, vous en faites ce que vous voulez.
% J'ai modifi√© et partag√© ce gabarit afin d'√©pargner √ d'autres 
% d'interminables heures √ modifier des gabarits d'articles anglais. 
% 
% L'ajout d'une table des mati√®res et une bibliographie a √©t√© ajout√©e,
% rendant le gabarit plus ajust√© aux besoins de plusieurs.
%
% Pour retrouv√© le gabarit original, veuillez t√©l√©charger les
% documents suivants: llncs2e.zip (.cls et autres) et 
% typeinst.zip (.tex). Les documents ci-haut mentionn√©s ne sont pas 
% disponibles au m√™me endroit, alors je vous invite √ fouiller le web. 
%
% Pour l'instant (02-2016) ils sont disponibles tous deux ici :
%
% http://kawahara.ca/springer-lncs-latex-template/
%
% Netkompt
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%% file typeinst.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is the LaTeX source for the instructions to authors using
% the LaTeX document class 'llncs.cls' for contributions to
% the Lecture Notes in Computer Sciences series.
% http://www.springer.com/lncs       Springer Heidelberg 2006/05/04
%
% It may be used as a template for your own input - copy it
% to a new file with a new name and use it as the basis
% for your article.
%
% NB: the document class 'llncs' has its own and detailed documentation, see
% ftp://ftp.springer.de/data/pubftp/pub/tex/latex/llncs/latex2e/llncsdoc.pdf
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% !BIB TS-program = biber

\documentclass[runningheads, a4paper]{llncs}

\usepackage[utf8]{inputenc}

\usepackage[
backend=biber,
style=alphabetic,
sorting=ynt
]{biblatex}
\addbibresource{references.bib}

\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{dsfont}
\setcounter{tocdepth}{3}
\usepackage{graphicx}

\usepackage{url}
\newcommand{\keywords}[1]{\par\addvspace\baselineskip
\noindent\keywordname\enspace\ignorespaces#1}

\begin{document}

\mainmatter 

\title{Stein Variational Importance Sampling}

\author{Sky}

\institute{}

\maketitle

\medskip

\section{RBF Kernel}
According to section 5 of the paper, the authors use the RBF kernel defined as $k(x, x') = \exp(-||x -x'||^2/h)$ with $h$ being the kernel bandwidth. $h$ is defined as $\textrm{med}^2 / (2 \log (|A| +1))$ where $\textrm{med}$ is the median of the pairwise distances between the leader particles and $|A|$ is the number of leader particles. (Note: The authors took the median of the squared pairwise distances between the leader particles.)

\subsection{Construct mapping for leaders}
We first construct the map using the leader particles $x^{\ell}_A$ with $\ell$ representing the $\ell$th iteration.  The pairwise distances $||x - x'||$ are already computed when defining the kernel.\\

\noindent
Compute the kernel by expanding the numerator that is exponentiated:
\begin{align}
||x_A - x_A'||^2
&= x_A^T x_A - 2 x_A^T x_A' + x_A'^T x_A'
\end{align}
Find the median pairwise distance by taking the square root of $||x_A - x_A'||^2$ and checking if it is odd or even. If it is odd, pick the median else take the mean of the two middle values. Square the median and divide by $2 \log (|A| +1)$. With the above, we have the kernel of the leader particles.\\

\noindent
Assume that we have $\nabla \log p(x^{\ell}_A)$ obtained using automatic differentiation. Automatic differentiation in Tensorflow is implemented such that it accumulates the variable we are differentiating by. Since the operation $\nabla \log p(x^{\ell}_A)$ only accumulates each particle once, it is safe to employ automatic differentiation here.\\

\noindent
However, computing the Jacobian of the kernel $\nabla_{x_A} k(x_A, x_A')$ leads to accumulation of each particle $|A|$ times so we have to do it by hand:
\begin{align}
\nabla_{x_A} k(x_A, x_A')
&= \nabla_{x_A} \exp (- (x_A^T x_A - 2 x_A^T x_A' + x_A'^T x_A') / h) \textrm{ with $h$ being constant}\\
&= -\frac{2(x_A -  x_A')}{h} \exp (- ||x_A - x_A'||^2 / h)
\end{align}

\noindent
Since the update $\phi_A^{\ell+1}$ consist of two terms being added together and we are performing a sum over them, the operations are commutative so we can split it up into adding the sum of one term to the sum of the other. Hence $\phi_A^{\ell+1}(\cdot)$ is equivalent to $\frac{1}{|A|} \{k(x_A, \cdot)^T \nabla \log p(x_A) + \sum_j -\frac{2(x_{A_j} - \cdot)}{h} \exp (- ||x_{A_j} - \cdot||^2 / h)\}$.

\subsection{Construct mapping for followers}
We do the same for the followers:
\begin{itemize}
\item $||x_A - x_B||^2 = x_A^T x_A - 2 x_A^T x_B + x_B^T x_B$
\item $\nabla_{x_A} k(x_A, x_B) = -\frac{2(x_A -  x_B)}{h} \exp (- ||x_A - x_B||^2 / h)$
\item $\phi_B^{\ell+1}(\cdot) = \frac{1}{|A|} \{k(x_A, \cdot)^T \nabla \log p(x_A) + \sum_j -\frac{2(x_{A_j} - \cdot)}{h} \exp (- ||x_{A_j} - \cdot||^2 / h)\}$
\end{itemize}

\subsection{Update leaders and followers}
Update $\phi_A$ and $\phi_B$ to both leader and follower particle by adding $\epsilon * \phi$ to them.

\subsection{Calculate density values of followers}
We now compute $\nabla _{x_{B_i}} \phi_B(x_{B_i}) = \frac{1}{A} \sum_{A} [\nabla_{x_A} \log p(x_{B_i})^T \nabla_{x_{B_i}}k(x_A, x_{B_i}) + \nabla_{x_A} \nabla_{x_{B_i}} k(x_A, x_{B_i})]$. Like before but with a slight tweak, $\nabla_{x_{B_i}} k(x_A , x_{B_i})$ is:
\begin{align}
\nabla_{x_{B_i}} \exp (- (x_A^T x_A - 2 x_A^T x_{B_i} + x_{B_i}^T x_{B_i}) / h)
&= \frac{2(x_A -  x_{B_i})}{h} \exp (- ||x_A - x_{B_i}||^2 / h)
\end{align}  
Shifting our focus to the $\nabla_{x_A}\nabla_{x_{B_i}} k(x_A, x_{B_i})$ term, we begin with the above result\footnote{http://mlg.eng.cam.ac.uk/mchutchon/DifferentiatingGPs.pdf}We
\begin{align}
&\nabla_{x_A}\nabla_{x_{B_i}} k(x_A, x_{B_i}) \\ 
&= \nabla_{x_A} \frac{2(x_A -  x_{B_i})}{h} \exp (- ||x_A - x_{B_i}||^2 / h) \\
&= \frac{2}{h} [\nabla_{x_A} x_A  \exp (- ||x_A - x_{B_i}||^2 / h) + \nabla_{x_A} x_{B_i} \exp (- ||x_A - x_{B_i}||^2 / h)] \\
&= \frac{2}{h} [I -  \frac{2(x_A -  x_{B_i})}{h} x_A^T -  \frac{2(x_A -  x_{B_i})}{h} x_{B_i}^T]\exp (- ||x_A - x_{B_i}||^2 / h) \\
&= \frac{2}{h} [I -  \frac{2}{h}(x_A -  x_{B_i})(x_A - x_{B_i})^T]\exp (- ||x_A - x_{B_i}||^2 / h)
\end{align}
Hence, we have
\begin{align}
\nabla_{x_{B_i}} \phi_B(x_{B_i}) 
&= \frac{1}{|A|} \sum_{j \in  A} [\nabla_{x_{A_j}} \log p(x_{A_j})^T \frac{2(x_{A_j} -  x_{B_i})}{h} \exp (- ||x_{A_j} - x_{B_i}||^2 / h)) \nonumber \\
&+ \frac{2}{h} (I -  \frac{2}{h}(x_{A_j} -  x_{B_i})(x_{A_j} - x_{B_i})^T)\exp (- ||x_{A_j} - x_{B_i}||^2 / h)] \\ 
&= \frac{1}{|A|} [\nabla_{x_A} \log p(x_A)^T \cdot \textrm{diag}(\exp(- ||x_A - x_{B_i}||^2 / h))  \cdot \frac{2(x_A -  x_{B_i})}{h} \nonumber \\ 
&+ \left(\frac{2}{h} \cdot \sum_{j \in A}  I \cdot \exp (- ||x_{A_j} - x_{B_i}||^2 / h)\right) \nonumber \\
&- \frac{2(x_A -  x_{B_i})}{h}^T \cdot \textrm{diag}(\exp(- ||x_A - x_{B_i}||^2 / h)) \cdot \frac{2(x_A -  x_{B_i})}{h}]
\end{align}

\subsection{Update density values of followers}
With $\nabla_{x_{B_i}} \phi_B(x_{B_i})$ in hand, we update the density values using the last equation of the Stein IS algorithm by using Tensorflow to compute the absolute determinant of $I + \epsilon \nabla_{x_{B_i}} \phi_B(x_{B_i})$ and multiplying the current density values by the inverse of the result. 

\newpage

\section{Fisher Kernel}
Instead of using the RBF kernel whose weaknesses are detailed in Zhuo et. al\footnote{https://arxiv.org/pdf/1711.04425.pdf}, we propose the use of the Fisher kernel which requires less parameter tuning (and could possibly sidestep the issues highlighted in Zhuo et. al) and might have better properties. The Fisher kernel is formally defined as $k(x, x') = \nabla_{\theta} \log(p(x|\theta) I^{-1} \nabla_{\theta} \log(p(x'|\theta)^T$ where $I$ is the Fisher matrix. In practice, the Fisher matrix is not computed to reduce computational cost, leading to the requirement that resulting kernel be normalised\footnote{http://qwone.com/~jason/writing/normalizeKernel.pdf} and made PSD as SVGD exploits the RHKS property for closed-form updates. We follow the format defined by the previous section.

\subsection{Construct mapping for leaders}
Like before, we can assume that $\nabla_x \log p(x_A^\ell|\theta)$ is obtained through automatic differentiation. However the issue this time is TensorFlow being unable to differentiate through a mixture model which requires analytical expressions for each parameter for the Gaussian Mixture Model\footnote{Checked against Matrix Cookbook} and $\nabla_x \nabla_{\theta} \log p(x_A^\ell|\theta)$ for $\nabla k(x, x')$.\\

\noindent
\textbf{Differentials of GMM loglikelihood}\\
Take $p(x|\theta) = \sum_m w_i p(x|\theta_i)$ where $\theta = \{w, \mu, \sigma^2\}$ with $m$ Gaussians.\\
Correspondingly, $\log p(x|\theta) = \log \sum_m w_i p(x|\theta_i)$.\\
Differentiating w.r.t. $w_i$, 
\begin{align}
\nabla_{w_i} \log p(x|\theta)
&= \frac{1}{\sum_m w_i p(x|\theta_i)} \nabla_{w_i} \sum_m w_i p(x|\theta_i)\\
&= \frac{w_i p(x|\theta_i)}{\sum_m w_i p(x|\theta_i)} \frac{1}{w_i}\\
\end{align}
\noindent
Differentiating w.r.t. $\mu_i$, 
\begin{align}
\nabla_{\mu_i} \log p(x|\theta)
&= \frac{1}{\sum_m w_i p(x|\theta_i)} \nabla_{\mu_i} \sum_m w_i p(x|\theta_i)\\
&= \frac{w_i p(x|\theta_i)}{\sum_m w_i p(x|\theta_i)} \left(\frac{x - \mu_i}{\sigma_i^2}\right)\\
\end{align}
\noindent
Differentiating w.r.t. $\sigma_i^2$\ \footnote{http://www.vlfeat.org/api/fisher-derivation.html}\footnote{https://hal.inria.fr/hal-00830491/PDF/journal.pdf}, 
\begin{align}
\nabla_{\sigma_i^2} \log p(x|\theta)
&= \frac{1}{\sum_m w_i p(x|\theta_i)} \nabla_{\sigma_i^2} \sum_m w_i p(x|\theta_i)\\
&= \frac{w_i p(x|\theta_i)}{\sum_m w_i p(x|\theta_i)}\ \underbrace{\frac{1}{2} \left(-\frac{1}{\sigma_i^2} + \left(\frac{x - \mu_i}{\sigma_i^2}\right)^2 \right)}_{\xi}
\end{align}
\noindent
%Let $z = \sum_m w_i p(x|\theta_i)$.\\
%Differentiating $\nabla_{w_i} \log z$ w.r.t $x$,
%\begin{align}
%\nabla_{x} \frac{1}{z} p(x|\theta_i)
%&= \nabla_{x} z^{-1} p(x|\theta_i) + z^{-1} \nabla_x p(x|\theta_i)\\
%&= -\frac{1}{z^2} \nabla_x z\ p(x|\theta_i) + \frac{1}{z} -\left(\frac{x - \mu_i}{\sigma_i^2}\right) p(x|\theta_i)\\
%&= \frac{1}{z} \underbrace{\frac{1}{z} \left[\sum_m \left(\frac{x - \mu_i}{\sigma_i^2}\right) w_i p(x|\theta_i)\right]}_{\zeta} p(x|\theta_i) - \frac{1}{z} \left(\frac{x - \mu_i}{\sigma_i^2}\right) p(x|\theta_i)\\
%&= \frac{1}{z}  \left[\zeta - \left(\frac{x - \mu_i}{\sigma_i^2}\right)\right] p(x|\theta_i)\\
%\end{align}
%\noindent
%Differentiating $\nabla_{\mu_i} \log z$ w.r.t $x$,
%\begin{align}
%&\nabla_{x} \frac{1}{z} w_i p(x|\theta_i) \left(\frac{x - \mu_i}{\sigma_i^2}\right)\\
%&= \nabla_{x} z^{-1} w_i p(x|\theta_i) \left(\frac{x - \mu_i}{\sigma_i^2}\right) + \frac{1}{z} w_i \left[\nabla_x  p(x|\theta_i) \left(\frac{x - \mu_i}{\sigma_i^2}\right) + p(x|\theta_i) \nabla_x \left(\frac{x - \mu_i}{\sigma_i^2}\right)\right]\\
%&= \frac{1}{z} \zeta^T \ w_i p(x|\theta_i) \left(\frac{x - \mu_i}{\sigma_i^2}\right) + \frac{1}{z} \left[\frac{1}{\sigma_i^2} - \left(\frac{x - \mu_i}{\sigma_i^2}\right)^T \left(\frac{x - \mu_i}{\sigma_i^2}\right)\right] w_i p(x|\theta_i)\\
%&= \frac{1}{z} \left\{\zeta^T \left(\frac{x - \mu_i}{\sigma_i^2}\right) + \left[\frac{1}{\sigma_i^2} - \left(\frac{x - \mu_i}{\sigma_i^2}\right)^T  \left(\frac{x - \mu_i}{\sigma_i^2}\right)\right]\right\} w_i p(x|\theta_i)\\
%\end{align}
%\noindent
%Differentiating $\nabla_{\sigma_i^2} \log z$ w.r.t $x$,
%\begin{align}
%&\nabla_{x} \frac{1}{z} w_i p(x|\theta_i) \xi\\
%&= \nabla_{x} z^{-1} w_i p(x|\theta_i) \xi + \frac{1}{z} w_i \left[\nabla_x  p(x|\theta_i) \xi + p(x|\theta_i) \frac{1}{2} \nabla_x \frac{(x - \mu_i)^2}{\sigma_i^4}\right]\\
%&= \frac{1}{z} \zeta \ w_i p(x|\theta_i) \xi + \frac{1}{z} \left[-w_i p(x|\theta_i) \xi \left(\frac{x - \mu_i}{\sigma_i^2}\right) + w_i p(x|\theta_i) \frac{1}{\sigma_i^2} \left(\frac{x - \mu_i}{\sigma_i^2}\right)\right]\\
%&= \frac{1}{z} \left\{\zeta \xi + \left[\left(\frac{1}{\sigma_i^2} - \xi \right) \left(\frac{x - \mu_i}{\sigma_i^2}\right)\right]\right\} w_i p(x|\theta_i)\\
%\end{align}
\textbf{Second Order Differentials of GMM loglikelihood}\\
To ensure that the second order differentials are right, we start by differentiating an entry in the Fisher kernel and then generalise from the obtained results.\\ \\
Differentiating $k_{w_i}(x_1, x_2)$ w.r.t. $x$, 
\begin{align}
\nabla_{x_1} k_{w_i}(x_1, x_2)
&=\nabla_{x_1} \frac{w_i^2 p(x_1|\theta_i) p(x_2|\theta_i)}{\sum w_i p(x_1|\theta_i) \sum w_i p(x_2|\theta_i)}\\
&= \frac{\sum \left(\frac{x_1 - \mu_i}{\sigma_i^2}\right) w_i p(x_1|\theta_i)}{(\sum w_i p(x_1|\theta_i))^2} \frac{w_i^2 p(x_1|\theta_i) p(x_2|\theta_i)}{\sum w_i p(x_2|\theta_i)}\\ 
&\ \ \ \ - \frac{w_i^2 p(x_2|\theta_i)}{\sum w_i p(x_1|\theta_i) \sum w_i p(x_2|\theta_i)} \left(\frac{x_1 - \mu_i}{\sigma_i^2}\right) p(x_1|\theta_i)
\end{align}
We then separate the contribution of $\nabla_{w_i} \log \sum w_i p(x_2|\theta_i)$ from the above to get $\nabla_{x_1} \nabla_{w_i} \log \sum w_i p(x_1|\theta_i)^T$
\begin{align}
&\nabla_{x_1} \nabla_{w_i} \log \sum w_i p(x_1|\theta_i)^T \nonumber \\
&=\frac{w_i p(x_1|\theta_i)}{\sum w_i p(x_1|\theta_i)} \left[\frac{\sum \left(\frac{x_1 - \mu_i}{\sigma_i^2}\right) w_i p(x_1|\theta_i)}{\sum w_i p(x_1|\theta_i)} - \left(\frac{x_1 - \mu_i}{\sigma_i^2}\right)\right]
\end{align}
Its transpose is
\begin{align}
&\nabla_{x_1} \nabla_{w_i} \log \sum w_i p(x_1|\theta_i) \nonumber \\
&=\frac{w_i p(x_1|\theta_i)}{\sum w_i p(x_1|\theta_i)} \left[\frac{\sum \left(\frac{x_1 - \mu_i}{\sigma_i^2}\right) w_i p(x_1|\theta_i)}{\sum w_i p(x_1|\theta_i)} - \left(\frac{x_1 - \mu_i}{\sigma_i^2}\right)\right]^T
\end{align}

Differentiating $k_{\mu_i}(x_1, x_2)$ w.r.t. $x$, 
\begin{align}
\nabla_{x_1} k_{\mu_i}(x_1, x_2)
&=\nabla_{x_1} \frac{w_i^2 p(x_1|\theta_i) p(x_2|\theta_i)}{\sum w_i p(x_1|\theta_i) \sum w_i p(x_2|\theta_i)} \left(\frac{x_1 - \mu_i}{\sigma_i^2}\right)^T \left(\frac{x_2 - \mu_i}{\sigma_i^2}\right)\\
&= \frac{\sum \left(\frac{x_1 - \mu_i}{\sigma_i^2}\right) w_i p(x_1|\theta_i)}{(\sum w_i p(x_1|\theta_i))^2} \frac{w_i^2 p(x_1|\theta_i) p(x_2|\theta_i)}{\sum w_i p(x_2|\theta_i)} \left(\frac{x_1 - \mu_i}{\sigma_i^2}\right)^T \left(\frac{x_2 - \mu_i}{\sigma_i^2}\right)\\
&\ \ \ \ - \frac{w_i^2 p(x_2|\theta_i)}{\sum w_i p(x_1|\theta_i) \sum w_i p(x_2|\theta_i)} \left(\frac{x_1 - \mu_i}{\sigma_i^2}\right) p(x_1|\theta_i) \left(\frac{x_1 - \mu_i}{\sigma_i^2}\right)^T \left(\frac{x_2 - \mu_i}{\sigma_i^2}\right)\\
&\ \ \ \ + \frac{w_i^2 p(x_1|\theta_i) p(x_2|\theta_i)}{\sum w_i p(x_1|\theta_i) \sum w_i p(x_2|\theta_i)} \frac{1}{\sigma_i^2} \left(\frac{x_2 - \mu_i}{\sigma_i^2}\right)
\end{align}
We then separate the contribution of $\nabla_{\mu_i} \log \sum w_i p(x_2|\theta_i)$ from the above to get $\nabla_{x_1} \nabla_{\mu_i} \log \sum w_i p(x_1|\theta_i)^T$
\begin{align}
&\nabla_{x_1} \nabla_{\mu_i} \log \sum w_i p(x_1|\theta_i)^T \nonumber \\
&=\frac{w_i p(x_1|\theta_i)}{\sum w_i p(x_1|\theta_i)} \left[\left(\frac{\sum \left(\frac{x_1 - \mu_i}{\sigma_i^2}\right) w_i p(x_1|\theta_i)}{\sum w_i p(x_1|\theta_i)} - \left(\frac{x_1 - \mu_i}{\sigma_i^2}\right)\right)\left(\frac{x_1 - \mu_i}{\sigma_i^2}\right)^T + \frac{1}{\sigma_i^2} \right]
\end{align}
where $\sigma_i^2$ is a $d \times d$ diagonal matrix instead of a $d \times 1$ vector.
Its transpose is
\begin{align}
&\nabla_{x_1} \nabla_{\mu_i} \log \sum w_i p(x_1|\theta_i) \nonumber \\
&=\frac{w_i p(x_1|\theta_i)}{\sum w_i p(x_1|\theta_i)} \left[\left(\frac{x_1 - \mu_i}{\sigma_i^2}\right) \left(\frac{\sum \left(\frac{x_1 - \mu_i}{\sigma_i^2}\right) w_i p(x_1|\theta_i)}{\sum w_i p(x_1|\theta_i)} - \left(\frac{x_1 - \mu_i}{\sigma_i^2}\right)\right)^T + \frac{1}{\sigma_i^2} \right]
\end{align}
Differentiating $k_{\sigma_i^2}(x_1, x_2)$ w.r.t. $x$, 
\begin{align}
\nabla_{x_1} k_{\sigma_i^2}(x_1, x_2)
&=\nabla_{x_1} \frac{w_i^2 p(x_1|\theta_i) p(x_2|\theta_i)}{\sum w_i p(x_1|\theta_i) \sum w_i p(x_2|\theta_i)} \xi_1^T \xi_2\\
&= \frac{\sum \left(\frac{x_1 - \mu_i}{\sigma_i^2}\right) w_i p(x_1|\theta_i)}{(\sum w_i p(x_1|\theta_i))^2} \frac{w_i^2 p(x_1|\theta_i) p(x_2|\theta_i)}{\sum w_i p(x_2|\theta_i)} \xi_1^T \xi_2\\
&\ \ \ \ - \frac{w_i^2 p(x_2|\theta_i)}{\sum w_i p(x_1|\theta_i) \sum w_i p(x_2|\theta_i)} \left(\frac{x_1 - \mu_i}{\sigma_i^2}\right) p(x_1|\theta_i) \xi_1^T \xi_2\\
&\ \ \ \ + \frac{w_i^2 p(x_1|\theta_i) p(x_2|\theta_i)}{\sum w_i p(x_1|\theta_i) \sum w_i p(x_2|\theta_i)} \left(\textrm{diag}\left(\frac{x_1 - \mu_i}{\sigma_i^2}\right)\frac{1}{\sigma_i^2}\right)^T \xi_2
\end{align}
We then separate the contribution of $\nabla_{\sigma_i^2} \log \sum w_i p(x_2|\theta_i)$ from the above to get $\nabla_{x_1} \nabla_{\sigma_i^2} \log \sum w_i p(x_1|\theta_i)^T$
\begin{align}
&\nabla_{x_1} \nabla_{\sigma_i^2} \log \sum w_i p(x_1|\theta_i)^T \nonumber \\
&=\frac{w_i p(x_1|\theta_i)}{\sum w_i p(x_1|\theta_i)} \left[\left(\frac{\sum \left(\frac{x_1 - \mu_i}{\sigma_i^2}\right) w_i p(x_1|\theta_i)}{\sum w_i p(x_1|\theta_i)} - \left(\frac{x_1 - \mu_i}{\sigma_i^2}\right)\right) \xi_1^T + \frac{1}{\sigma_i^2}\textrm{diag}\left(\frac{x_1 - \mu_i}{\sigma_i^2}\right) \right]
\end{align}
where $\sigma_i^2$ is a $d \times d$ diagonal matrix instead of a $d \times 1$ vector.
Its transpose is
\begin{align}
&\nabla_{x_1} \nabla_{\sigma_i^2} \log \sum w_i p(x_1|\theta_i) \nonumber \\
&=\frac{w_i p(x_1|\theta_i)}{\sum w_i p(x_1|\theta_i)} \left[\xi_1 \left(\frac{\sum \left(\frac{x_1 - \mu_i}{\sigma_i^2}\right) w_i p(x_1|\theta_i)}{\sum w_i p(x_1|\theta_i)} - \left(\frac{x_1 - \mu_i}{\sigma_i^2}\right)\right)^T + \textrm{diag}\left(\frac{x_1 - \mu_i}{\sigma_i^2}\right)\frac{1}{\sigma_i^2} \right]
\end{align}
\end{document}
