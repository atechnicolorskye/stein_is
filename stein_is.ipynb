{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.distributions as ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GMM(object):\n",
    "    def __init__(self, mu, sigma, weights, dim):\n",
    "        # Required parameters \n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.weights = weights\n",
    "        self.dim = dim\n",
    "        \n",
    "    def log_px(self, x):\n",
    "        # log_px = log(sum(exp(log(w_i) + log(p_i(x)))))\n",
    "        log_px = []\n",
    "        for i in range(weights.shape[0]):\n",
    "            mu_, sigma_ = self.mu[i] * tf.ones(dim), self.sigma[i] * tf.ones(dim)\n",
    "            mvn = ds.MultivariateNormalDiag(loc=mu_, scale_diag=sigma_)\n",
    "            # Calculate log_px for each component\n",
    "            log_px_i = tf.reduce_logsumexp(mvn.log_prob(x)) + tf.log(tf.to_float(weights[i]))\n",
    "            log_px.append(log_px_i)\n",
    "        return tf.reduce_logsumexp(log_px)\n",
    "    \n",
    "    def d_log_px(self, x):\n",
    "        # d_log_px = 1 / exp(log(sum(exp(log(w_i) + log(p_i(x)))))) \n",
    "        #            * sum(exp(log(w_i) + log(p_i(x)) + log(-(x - mu)/sigma^2)))\n",
    "        # Use symbolic differentiation instead\n",
    "        log_px = self.log_px(x)\n",
    "        return tf.gradients(log_px, [x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mu = np.array([1., -1.]); sigma = np.sqrt(np.array([0.1, 0.05])); weights = np.array([1./3, 2./3]); dim=6\n",
    "gmm = GMM(mu, sigma, weights, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def median(x):\n",
    "    x = tf.reshape(x, [-1])\n",
    "    med = tf.floordiv(tf.shape(x)[0], 2)\n",
    "    check_parity = tf.equal(tf.to_float(med), tf.divide(tf.to_float(tf.shape(x)[0]), 2.))\n",
    "    def is_true():\n",
    "        return tf.reduce_sum(tf.nn.top_k(x, med+1).values[-2:]) / 2.\n",
    "    def is_false():\n",
    "        return tf.nn.top_k(x, med+1).values[-1]\n",
    "    return tf.cond(check_parity, is_true, is_false) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class stein_is(object):\n",
    "    def __init__(self, gmm_model, mu, sigma, dim, n_leaders, n_followers, step_size=0.01): # n_trials, step_size=0.01):\n",
    "        # Required parameters\n",
    "        self.gmm_model = gmm_model\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.dim = dim\n",
    "        self.n_leaders = n_leaders\n",
    "        self.n_followers = n_followers\n",
    "        # self.n_trials = n_trials\n",
    "        self.step_size = step_size\n",
    "        self.eps = 1e-10\n",
    "        \n",
    "        # Set seed\n",
    "        seed = 30\n",
    "        \n",
    "        # Intialisation\n",
    "        self.B, self.B_density, self.A = self.initialise_variables()\n",
    "        self.pB = self.gmm_model.log_px(self.B)\n",
    "        \n",
    "        # Register functions for debugging\n",
    "        # self.k_A_A, self.sum_grad_A_k_A_A, self.A_Squared, self.h_0 = self.construct_map()\n",
    "        # self.k_A_B, self.sum_grad_A_k_A_B, self.grad_A_grad_B_k_A_B, self.grad_B_k_A_B = self.apply_map()        \n",
    "        self.A, self.B, self.phi_B, self.grad_B_phi_B = self.svgd_update()\n",
    "        self.q_density = self.density_update()\n",
    "        \n",
    "        \n",
    "    def initialise_variables(self):\n",
    "        init_distribution = tf.contrib.distributions.MultivariateNormalDiag(self.mu * tf.ones(dim), self.sigma * tf.ones(dim))\n",
    "        \n",
    "        # followers = tf.reshape(init_distribution.sample(self.n_trials * self.n_followers, seed=123), [self.n_trials, self.n_followers, self.h_dim] \n",
    "        # leaders = tf.reshape(init_distribution.sample(self.n_trials * self.n_leaders, seed=123), [self.n_trials, self.n_leaders, self.h_dim] \n",
    "        \n",
    "        followers = tf.reshape(init_distribution.sample(self.n_followers, seed=123), [self.n_followers, self.dim]) \n",
    "        q_density = init_distribution.log_prob(followers)\n",
    "        leaders = tf.reshape(init_distribution.sample(self.n_leaders, seed=123), [self.n_leaders, self.dim])\n",
    "                           \n",
    "        return followers, q_density, leaders\n",
    "                             \n",
    "    def construct_map(self):\n",
    "        # Calculate ||leader - leader'||^2/h_0, refer to leader as A as in SteinIS\n",
    "        x2_A_A_T = tf.multiply(2., tf.matmul(self.A, tf.transpose(self.A)))\n",
    "        A_Squared = tf.reduce_sum(tf.square(self.A), 1)\n",
    "        A_Distance = tf.add(tf.subtract(A_Squared, x2_A_A_T), tf.transpose(A_Squared))   \n",
    "        # h_0 = tf.divide(tf.add(median(A_Distance), self.eps), 2. * (tf.log(tf.cast(self.n_leaders, tf.float32)) + 1.))\n",
    "        h_0 = tf.divide(median(A_Distance), 2. * (tf.log(tf.to_float(self.n_leaders)) + 1.))\n",
    "        k_A_A = tf.exp(-tf.div(A_Distance, tf.square(h_0)))\n",
    "        sum_grad_A_k_A_A = tf.reduce_sum(tf.gradients(k_A_A, [self.A]), 1)\n",
    "        return k_A_A, sum_grad_A_k_A_A, A_Squared, h_0\n",
    "    \n",
    "    def apply_map(self):\n",
    "        # Calculate ||leader - follower||^2/h_0, refer to follower as B as in SteinIS\n",
    "        x2_A_B_T = tf.multiply(2., tf.matmul(self.A, tf.transpose(self.B)))\n",
    "        B_Squared = tf.reduce_sum(tf.square(self.B), 1)\n",
    "        A_B_Distance  = tf.add(tf.subtract(self.A_Squared, x2_A_B_T), B_Squared)\n",
    "        k_A_B = tf.exp(-tf.div(A_B_Distance, tf.square(self.h_0)))\n",
    "        sum_grad_A_k_A_B = tf.reduce_sum(tf.gradients(k_A_B, [self.A]), 1)\n",
    "        return k_A_B, sum_grad_A_k_A_B \n",
    "                    \n",
    "    def svgd_update(self):\n",
    "        self.k_A_A, self.sum_grad_A_k_A_A, self.A_Squared, self.h_0 = self.construct_map()\n",
    "        self.k_A_B, self.sum_grad_A_k_A_B = self.apply_map()\n",
    "        self.d_log_pA = self.gmm_model.d_log_px(self.A)[0]\n",
    "        sum_d_log_pA_T_k_A_A = tf.reduce_sum(tf.matmul(self.k_A_A, self.d_log_pA), 0)       \n",
    "        phi_A = (1. / tf.to_float(self.n_leaders)) * tf.add(sum_d_log_pA_T_k_A_A, self.sum_grad_A_k_A_A)\n",
    "        A = tf.add(self.A, self.step_size * phi_A)  \n",
    "        sum_d_log_pA_T_k_A_B = tf.reduce_sum(tf.matmul(self.k_A_B, self.d_log_pA), 0)       \n",
    "        phi_B = (1. / tf.to_float(self.n_leaders)) * tf.add(sum_d_log_pA_T_k_A_B, self.sum_grad_A_k_A_B)\n",
    "        B = tf.add(self.B, self.step_size * phi_B) \n",
    "        grad_B_phi_B = tf.gradients(phi_B, [self.B])\n",
    "        return A, B, phi_B, grad_B_phi_B[0]\n",
    "    \n",
    "    def density_update(self):\n",
    "        I = tf.eye(self.dim)\n",
    "        inv_abs_det_I_grad_B_phi = tf.map_fn(lambda x: 1./tf.abs(tf.matrix_determinant(tf.add(I, x))), self.grad_B_phi_B)\n",
    "        return tf.multiply(self.B_density, inv_abs_det_I_grad_B_phi) \n",
    "    \n",
    "    def main(self, iteration):\n",
    "        for i in range(iteration):\n",
    "            self.A, self.B, self.phi_B, self.grad_B_phi_B = self.svgd_update()\n",
    "            self.q_density = self.density_update()\n",
    "            print 'Iteration ', str(i), ' done'\n",
    "        self.importance_weights = tf.divide(self.q_density, self.pB)\n",
    "        self.normalisation_constant = 1./tf.to_float(self.n_followers) * tf.reduce_sum(self.importance_weights)\n",
    "        self.final_B = self.B\n",
    "        return self.normalisation_constant\n",
    "#         return self.final_B, self.importance_weights, self.normalisation_constant\n",
    "#         return self.A, self.B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mu = 0.; sigma = 3.; dim = 6; n_leaders = 100; n_followers = 100;\n",
    "model = stein_is(gmm,  mu, sigma, dim, n_leaders, n_followers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  0  done\n",
      "Iteration  1  done\n",
      "Iteration  2  done\n",
      "Iteration  3  done\n",
      "Iteration  4  done\n",
      "Iteration  5  done\n",
      "Iteration  6  done\n",
      "Iteration  7  done\n",
      "Iteration  8  done\n",
      "Iteration  9  done\n",
      "Iteration  10  done\n",
      "Iteration  11  done\n",
      "Iteration  12  done\n",
      "Iteration  13  done\n",
      "Iteration  14  done\n",
      "Iteration  15  done\n",
      "Iteration  16  done\n",
      "Iteration  17  done\n",
      "Iteration  18  done\n",
      "Iteration  19  done\n",
      "Iteration  20  done\n",
      "Iteration  21  done\n",
      "Iteration  22  done\n",
      "Iteration  23  done\n",
      "Iteration  24  done\n",
      "Iteration  25  done\n",
      "Iteration  26  done\n",
      "Iteration  27  done\n",
      "Iteration  28  done\n",
      "Iteration  29  done\n",
      "Iteration  30  done\n",
      "Iteration  31  done\n",
      "Iteration  32  done\n",
      "Iteration  33  done\n",
      "Iteration  34  done\n",
      "Iteration  35  done\n",
      "Iteration  36  done\n",
      "Iteration  37  done\n",
      "Iteration  38  done\n",
      "Iteration  39  done\n",
      "Iteration  40  done\n",
      "Iteration  41  done\n",
      "Iteration  42  done\n",
      "Iteration  43  done\n",
      "Iteration  44  done\n",
      "Iteration  45  done\n",
      "Iteration  46  done\n",
      "Iteration  47  done\n",
      "Iteration  48  done\n",
      "Iteration  49  done\n",
      "Iteration  50  done\n",
      "Iteration  51  done\n",
      "Iteration  52  done\n",
      "Iteration  53  done\n",
      "Iteration  54  done\n",
      "Iteration  55  done\n",
      "Iteration  56  done\n",
      "Iteration  57  done\n",
      "Iteration  58  done\n",
      "Iteration  59  done\n",
      "Iteration  60  done\n",
      "Iteration  61  done\n",
      "Iteration  62  done\n",
      "Iteration  63  done\n",
      "Iteration  64  done\n",
      "Iteration  65  done\n",
      "Iteration  66  done\n",
      "Iteration  67  done\n",
      "Iteration  68  done\n",
      "Iteration  69  done\n",
      "Iteration  70  done\n",
      "Iteration  71  done\n",
      "Iteration  72  done\n",
      "Iteration  73  done\n",
      "Iteration  74  done\n",
      "Iteration  75  done\n",
      "Iteration  76  done\n",
      "Iteration  77  done\n",
      "Iteration  78  done\n",
      "Iteration  79  done\n",
      "Iteration  80  done\n",
      "Iteration  81  done\n",
      "Iteration  82  done\n",
      "Iteration  83  done\n",
      "Iteration  84  done\n",
      "Iteration  85  done\n",
      "Iteration  86  done\n",
      "Iteration  87  done\n",
      "Iteration  88  done\n",
      "Iteration  89  done\n",
      "Iteration  90  done\n",
      "Iteration  91  done\n",
      "Iteration  92  done\n",
      "Iteration  93  done\n",
      "Iteration  94  done\n",
      "Iteration  95  done\n",
      "Iteration  96  done\n",
      "Iteration  97  done\n",
      "Iteration  98  done\n",
      "Iteration  99  done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.82154536]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run([model.main(100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
