{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.distributions as ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def median(x):\n",
    "    x = tf.reshape(x, [-1])\n",
    "    med = tf.floordiv(tf.shape(x)[0], 2)\n",
    "    check_parity = tf.equal(tf.to_double(med), tf.divide(tf.to_double(tf.shape(x)[0]), 2.))\n",
    "    def is_true():\n",
    "        return 0.5 * tf.reduce_sum(tf.nn.top_k(x, med+1).values[-2:]) \n",
    "    def is_false():\n",
    "        return tf.nn.top_k(x, med+1).values[-1]\n",
    "    return tf.cond(check_parity, is_true, is_false) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GMM(object):\n",
    "    def __init__(self, mu, sigma, weights, dim):\n",
    "        # Required parameters \n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.weights = weights\n",
    "        self.dim = dim\n",
    "        \n",
    "    def log_px(self, x):\n",
    "        # log_px = log(sum(exp(log(w_i) + log(p_i(x)))))\n",
    "        log_px = []\n",
    "        for i in range(weights.shape[0]):\n",
    "            mu_, sigma_ = self.mu[i] * tf.ones(dim), self.sigma[i] * tf.ones(dim)\n",
    "            mvn = ds.MultivariateNormalDiag(loc=mu_, scale_diag=sigma_)\n",
    "            # Calculate log_px for each component\n",
    "            log_px_i = tf.reduce_logsumexp(mvn.log_prob(x)) + tf.log(tf.to_float(weights[i]))\n",
    "            log_px.append(log_px_i)\n",
    "        return tf.reduce_logsumexp(log_px)\n",
    "    \n",
    "    def d_log_px(self, x):\n",
    "        # d_log_px = 1 / exp(log(sum(exp(log(w_i) + log(p_i(x)))))) \n",
    "        #            * sum(exp(log(w_i) + log(p_i(x)) + log(-(x - mu)/sigma^2)))\n",
    "        # Use symbolic differentiation instead\n",
    "        log_px = self.log_px(x)\n",
    "        return tf.gradients(log_px, [x])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mu = np.array([1., -1.]); sigma = np.sqrt(np.array([0.1, 0.05])); weights = np.array([1./3, 2./3]); dim=6\n",
    "mu = np.array([[-.5], [.5], [-1.], [1.0], [-1.5], [1.5], [-2.0], [2.0], [-2.5], [2.5]]).astype(np.float32)\n",
    "sigma = 2 * np.ones(10).astype(np.float32)\n",
    "weights = (1/10.0 * np.ones(10)).astype(np.float32)\n",
    "dim = 2\n",
    "gmm = GMM(mu, sigma, weights, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SteinIS(object):\n",
    "    def __init__(self, gmm_model, mu, sigma, dim, n_leaders, n_followers, step_size_master=1., step_size_beta=0.35): # n_trials, step_size=0.01):\n",
    "        # Required parameters\n",
    "        self.gmm_model = gmm_model\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.dim = dim\n",
    "        # Check if it works \n",
    "        self.n_leaders = n_leaders\n",
    "        self.n_followers = n_followers\n",
    "        # self.n_trials = n_trials\n",
    "        self.step_size = 1\n",
    "        self.step_size_master = step_size_master\n",
    "        self.step_size_beta = step_size_beta\n",
    "        self.eps = 1e-10\n",
    "        \n",
    "        # Set seed\n",
    "        seed = 30\n",
    "        \n",
    "        # Intialisation\n",
    "        self.B, self.B_density, self.A = self.initialise_variables()\n",
    "        self.pB = self.gmm_model.log_px(self.B)\n",
    "        \n",
    "        # Register functions for debugging\n",
    "        self.k_A_A, self.sum_grad_A_k_A_A, self.A_Squared, self.h = self.construct_map()\n",
    "        self.k_A_B, self.sum_grad_A_k_A_B = self.apply_map()        \n",
    "        self.n_A, self.n_B = self.svgd_update()\n",
    "        # self.q_density = self.density_update()\n",
    "        \n",
    "        \n",
    "    def initialise_variables(self):\n",
    "        init_distribution = tf.contrib.distributions.MultivariateNormalDiag(self.mu * tf.ones(dim), self.sigma * tf.ones(dim))\n",
    "        \n",
    "        # followers = tf.reshape(init_distribution.sample(self.n_trials * self.n_followers, seed=123), [self.n_trials, self.n_followers, self.h_dim] \n",
    "        # leaders = tf.reshape(init_distribution.sample(self.n_trials * self.n_leaders, seed=123), [self.n_trials, self.n_leaders, self.h_dim] \n",
    "        \n",
    "        followers = tf.reshape(init_distribution.sample(self.n_followers, seed=123), [self.n_followers, self.dim]) \n",
    "        q_density = init_distribution.log_prob(followers)\n",
    "        leaders = tf.reshape(init_distribution.sample(self.n_leaders, seed=123), [self.n_leaders, self.dim])                   \n",
    "        return followers, q_density, leaders\n",
    "                             \n",
    "    def construct_map(self):\n",
    "        # Calculate ||leader - leader'||^2/h_0, refer to leader as A as in SteinIS\n",
    "        x2_A_A_T = 2. * tf.matmul(self.A, tf.transpose(self.A)) # 100 x 100\n",
    "        A_Squared = tf.reduce_sum(tf.square(self.A), keep_dims=True, axis=1) # 100 x 1\n",
    "        A_A_Distance_Squared = A_Squared - x2_A_A_T + tf.transpose(A_Squared) # 100 x 100\n",
    "        h_num = tf.square(median(tf.sqrt(A_A_Distance_Squared)))\n",
    "        h_dem = 2. * tf.log(tf.to_float(self.n_leaders) + 1.)\n",
    "        h = tf.stop_gradient(h_num / h_dem)\n",
    "        k_A_A = tf.exp(-A_A_Distance_Squared / h)\n",
    "        # Can't use vanilla tf.gradients as it sums dy/dx wrt to dx, want sum dy/dx wrt to dy, tf.map_fn is not available\n",
    "        # tf.gradients also do not provide accurate gradients in this case\n",
    "        sum_grad_A_k_A_A = tf.stack([tf.matmul(tf.reshape(k_A_A[:, i], (1, -1)), -2 * (self.A - self.A[i]) / h) for i in range(self.n_leaders)])\n",
    "        return k_A_A, tf.squeeze(sum_grad_A_k_A_A), A_Squared, h\n",
    "        \n",
    "    def apply_map(self):\n",
    "        # Calculate ||leader - follower||^2/h_0, refer to follower as B as in SteinIS\n",
    "        x2_A_B_T = 2. * tf.matmul(self.A, tf.transpose(self.B))\n",
    "        B_Squared = tf.reduce_sum(tf.square(self.B), keep_dims=True, axis=1)\n",
    "        A_B_Distance_Squared = self.A_Squared - x2_A_B_T + tf.transpose(B_Squared)\n",
    "        k_A_B = tf.exp(-A_B_Distance_Squared / self.h)\n",
    "        sum_grad_A_k_A_B = tf.stack([tf.matmul(tf.reshape(k_A_B[:, i], (1, -1)), -2 * (self.A - self.B[i]) / self.h) for i in range(self.n_followers)])\n",
    "        return k_A_B, tf.squeeze(sum_grad_A_k_A_B)\n",
    "                    \n",
    "    def svgd_update(self):\n",
    "        self.k_A_A, self.sum_grad_A_k_A_A, self.A_Squared, self.h = self.construct_map()\n",
    "        self.k_A_B, self.sum_grad_A_k_A_B = self.apply_map()\n",
    "        self.d_log_pA = self.gmm_model.d_log_px(self.A)\n",
    "        sum_d_log_pA_T_k_A_A = tf.matmul(self.k_A_A, self.d_log_pA)\n",
    "        phi_A = (sum_d_log_pA_T_k_A_A + self.sum_grad_A_k_A_A) / self.n_leaders\n",
    "        A = self.A + self.step_size * phi_A  \n",
    "        sum_d_log_pA_T_k_A_B = tf.matmul(tf.transpose(self.k_A_B), self.d_log_pA)\n",
    "        phi_B = (sum_d_log_pA_T_k_A_B + self.sum_grad_A_k_A_B) / self.n_leaders\n",
    "        B = self.B + self.step_size * phi_B \n",
    "        # See http://mlg.eng.cam.ac.uk/mchutchon/DifferentiatingGPs.pdf\n",
    "        grad_B_sum_grad_A_k_A_B = []\n",
    "        for i in range(self.n_followers):\n",
    "            x = (self.A - self.B[i]) / self.h\n",
    "            term_2 = tf.matmul(tf.tranpose(x), tf.matmul(tf.diag(k_A_B[:, i]), x))\n",
    "            term_1 = (tf.reduce_sum(k_A_B[:, i] / self.h) * tf.eye(self.dim)\n",
    "            grad_B_sum_grad_A_k_A_B.append(4 * (term_1 - term_2))\n",
    "        return A, B #, grad_B_k_A_B, grad_B_sum_grad_A_k_A_B, grad_B_phi_B_\n",
    "    \n",
    "    def density_update(self):\n",
    "        I = tf.eye(self.dim)\n",
    "        inv_abs_det_I_grad_B_phi = tf.map_fn(lambda x: 1./tf.abs(tf.matrix_determinant(I + self.step_size * x)), self.grad_B_phi_B)\n",
    "        return tf.multiply(self.B_density, inv_abs_det_I_grad_B_phi) \n",
    "\n",
    "    def main(self, iteration):\n",
    "        for i in range(1, iteration+1):\n",
    "            self.step_size = self.step_size_master * (1. + i) ** (-self.step_size_beta)\n",
    "            self.A, self.B, self.phi_B, self.grad_B_phi_B = self.svgd_update()\n",
    "            self.q_density = self.density_update()\n",
    "            if i % 10 == 0:\n",
    "                self.pB = self.gmm_model.log_px(self.B)\n",
    "                self.importance_weights = self.pB / self.q_density\n",
    "                self.normalisation_constant = tf.reduce_sum(self.importance_weights) / self.n_followers\n",
    "                print 'Iteration ', str(i), ' done'\n",
    "        self.final_B = self.B\n",
    "        return self.normalisation_constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mu = 0.; sigma = 3.; dim = 6; n_leaders = 100; n_followers = 100;\n",
    "initial_mu = np.float32(0.)\n",
    "initial_sigma = np.float32(1.)\n",
    "n_leaders = 100\n",
    "n_followers = 200\n",
    "model = SteinIS(gmm, initial_mu, initial_sigma, dim, n_leaders, n_followers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "[w, x] = sess.run([model.n_A, model.n_B])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# w.shape, x[0].shape, y.shape, z[0].shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Look into using einsum https://www.tensorflow.org/api_docs/python/tf/einsum\n",
    "for i in range(n_followers):\n",
    "    if i == 0:\n",
    "        grad_B_phi_B = np.dot(w.T, x[i]).reshape((1, 2, 2))\n",
    "    else:\n",
    "        grad_B_phi_B = np.concatenate((grad_B_phi_B, np.dot(w.T, x[i]).reshape((1, 2, 2))), 0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "grad_B_phi_B.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 2), (200, 2))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.shape, x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
