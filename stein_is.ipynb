{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.distributions as ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GMM(object):\n",
    "    def __init__(self, mu, sigma, weights, dim):\n",
    "        # Required parameters \n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.weights = weights\n",
    "        self.dim = dim\n",
    "        \n",
    "    def log_px(self, x):\n",
    "        # log_px = log(sum(exp(log(w_i) + log(p_i(x)))))\n",
    "        log_px = []\n",
    "        for i in range(weights.shape[0]):\n",
    "            mu_, sigma_ = self.mu[i] * tf.ones(dim), self.sigma[i] * tf.ones(dim)\n",
    "            mvn = ds.MultivariateNormalDiag(loc=mu_, scale_diag=sigma_)\n",
    "            # Calculate log_px for each component\n",
    "            log_px_i = tf.reduce_logsumexp(mvn.log_prob(x)) + tf.log(tf.to_float(weights[i]))\n",
    "            log_px.append(log_px_i)\n",
    "        return tf.reduce_logsumexp(log_px)\n",
    "    \n",
    "    def d_log_px(self, x):\n",
    "        # d_log_px = 1 / exp(log(sum(exp(log(w_i) + log(p_i(x)))))) \n",
    "        #            * sum(exp(log(w_i) + log(p_i(x)) + log(-(x - mu)/sigma^2)))\n",
    "        # Use symbolic differentiation instead\n",
    "        log_px = self.log_px(x)\n",
    "        return tf.gradients(log_px, [x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mu = np.array([1., -1.]); sigma = np.sqrt(np.array([0.1, 0.05])); weights = np.array([1./3, 2./3]); dim=6\n",
    "gmm = GMM(mu, sigma, weights, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def median(x):\n",
    "    x = tf.reshape(x, [-1])\n",
    "    med = tf.floordiv(tf.shape(x)[0], 2)\n",
    "    check_parity = tf.equal(tf.to_float(med), tf.divide(tf.to_float(tf.shape(x)[0]), 2.))\n",
    "    def is_true():\n",
    "        return tf.reduce_sum(tf.nn.top_k(x, med+1).values[-2:]) / 2.\n",
    "    def is_false():\n",
    "        return tf.nn.top_k(x, med+1).values[-1]\n",
    "    return tf.cond(check_parity, is_true, is_false) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class stein_is(object):\n",
    "    def __init__(self, gmm_model, mu, sigma, dim, n_leaders, n_followers, step_size=0.01): # n_trials, step_size=0.01):\n",
    "        # Required parameters\n",
    "        self.gmm_model = gmm_model\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.dim = dim\n",
    "        self.n_leaders = n_leaders\n",
    "        self.n_followers = n_followers\n",
    "        # self.n_trials = n_trials\n",
    "        self.step_size = step_size\n",
    "        self.eps = 1e-10\n",
    "        \n",
    "        # Set seed\n",
    "        seed = 30\n",
    "        \n",
    "        # Intialisation\n",
    "        self.B, self.B_density, self.A = self.initialise_variables()\n",
    "        self.pB = self.gmm_model.log_px(self.B)\n",
    "        \n",
    "        # Register functions for debugging\n",
    "        # self.k_A_A, self.sum_grad_A_k_A_A, self.A_Squared, self.h_0 = self.construct_map()\n",
    "        # self.k_A_B, self.sum_grad_A_k_A_B, self.grad_A_grad_B_k_A_B, self.grad_B_k_A_B = self.apply_map()        \n",
    "        # self.A, self.B, self.phi_B, self.grad_B_phi_B = self.svgd_update()\n",
    "        # self.q_density = self.density_update()\n",
    "        \n",
    "        \n",
    "    def initialise_variables(self):\n",
    "        init_distribution = tf.contrib.distributions.MultivariateNormalDiag(self.mu * tf.ones(dim), self.sigma * tf.ones(dim))\n",
    "        \n",
    "        # followers = tf.reshape(init_distribution.sample(self.n_trials * self.n_followers, seed=123), [self.n_trials, self.n_followers, self.h_dim] \n",
    "        # leaders = tf.reshape(init_distribution.sample(self.n_trials * self.n_leaders, seed=123), [self.n_trials, self.n_leaders, self.h_dim] \n",
    "        \n",
    "        followers = tf.reshape(init_distribution.sample(self.n_followers, seed=123), [self.n_followers, self.dim]) \n",
    "        q_density = init_distribution.log_prob(followers)\n",
    "        leaders = tf.reshape(init_distribution.sample(self.n_leaders, seed=123), [self.n_leaders, self.dim])\n",
    "                           \n",
    "        return followers, q_density, leaders\n",
    "                             \n",
    "    def construct_map(self):\n",
    "        # Calculate ||leader - leader'||^2/h_0, refer to leader as A as in SteinIS\n",
    "        x2_A_A_T = tf.multiply(2., tf.matmul(self.A, tf.transpose(self.A)))\n",
    "        A_Squared = tf.reduce_sum(tf.square(self.A), 1)\n",
    "        A_Distance = tf.add(tf.subtract(A_Squared, x2_A_A_T), tf.transpose(A_Squared))   \n",
    "        # h_0 = tf.divide(tf.add(median(A_Distance), self.eps), 2. * (tf.log(tf.cast(self.n_leaders, tf.float32)) + 1.))\n",
    "        h_0 = tf.divide(median(A_Distance), 2. * (tf.log(tf.to_float(self.n_leaders)) + 1.))\n",
    "        k_A_A = tf.exp(-tf.div(A_Distance, tf.square(h_0)))\n",
    "        sum_grad_A_k_A_A = tf.reduce_sum(tf.gradients(k_A_A, [self.A]), 1)\n",
    "        return k_A_A, sum_grad_A_k_A_A, A_Squared, h_0\n",
    "    \n",
    "    def apply_map(self):\n",
    "        # Calculate ||leader - follower||^2/h_0, refer to follower as B as in SteinIS\n",
    "        x2_A_B_T = tf.multiply(2., tf.matmul(self.A, tf.transpose(self.B)))\n",
    "        B_Squared = tf.reduce_sum(tf.square(self.B), 1)\n",
    "        A_B_Distance  = tf.add(tf.subtract(self.A_Squared, x2_A_B_T), B_Squared)\n",
    "        k_A_B = tf.exp(-tf.div(A_B_Distance, tf.square(self.h_0)))\n",
    "        sum_grad_A_k_A_B = tf.reduce_sum(tf.gradients(k_A_B, [self.A]), 1)\n",
    "        return k_A_B, sum_grad_A_k_A_B \n",
    "                    \n",
    "    def svgd_update(self):\n",
    "        self.k_A_A, self.sum_grad_A_k_A_A, self.A_Squared, self.h_0 = self.construct_map()\n",
    "        self.k_A_B, self.sum_grad_A_k_A_B = self.apply_map()\n",
    "        self.d_log_pA = self.gmm_model.d_log_px(self.A)[0]\n",
    "        sum_d_log_pA_T_k_A_A = tf.reduce_sum(tf.matmul(self.k_A_A, self.d_log_pA), 0)       \n",
    "        phi_A = (1. / tf.to_float(self.n_leaders)) * tf.add(sum_d_log_pA_T_k_A_A, self.sum_grad_A_k_A_A)\n",
    "        A = tf.add(self.A, self.step_size * phi_A)  \n",
    "        sum_d_log_pA_T_k_A_B = tf.reduce_sum(tf.matmul(self.k_A_B, self.d_log_pA), 0)       \n",
    "        phi_B = (1. / tf.to_float(self.n_leaders)) * tf.add(sum_d_log_pA_T_k_A_B, self.sum_grad_A_k_A_B)\n",
    "        B = tf.add(self.B, self.step_size * phi_B) \n",
    "        grad_B_phi_B = tf.gradients(phi_B, [self.B])\n",
    "        return A, B, phi_B, grad_B_phi_B[0]\n",
    "    \n",
    "    def density_update(self):\n",
    "        I = tf.eye(self.dim)\n",
    "        inv_abs_det_I_grad_B_phi = tf.map_fn(lambda x: 1./tf.abs(tf.matrix_determinant(tf.add(I, x))), self.grad_B_phi_B)\n",
    "        return tf.multiply(self.B_density, inv_abs_det_I_grad_B_phi) \n",
    "    \n",
    "    def main(self, iteration):\n",
    "        for i in range(iteration):\n",
    "            self.A, self.B, self.phi_B, self.grad_B_phi_B = self.svgd_update()\n",
    "            self.q_density = self.density_update()\n",
    "            print 'Iteration ', str(i), ' done'\n",
    "        self.importance_weights = tf.divide(self.q_density, self.pB)\n",
    "        self.normalisation_constant = 1./tf.to_float(self.n_followers) * tf.reduce_sum(self.importance_weights)\n",
    "        self.final_B = self.B\n",
    "        return self.final_B, self.importance_weights, self.normalisation_constant                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 0.; sigma = 1.; dim = 6; n_leaders = 100; n_followers = 100;\n",
    "model = stein_is(gmm,  mu, sigma, dim, n_leaders, n_followers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ -1.03152409e+01,  -1.51769896e+01,  -6.91211319e+00,\n",
       "         -9.09164906e+00,  -1.66581357e+00,  -3.77475023e+00,\n",
       "         -3.40547347e+00,  -5.77358770e+00,  -5.48494339e-01,\n",
       "         -1.16953516e+01,  -4.43257749e-01,  -7.55881071e-01,\n",
       "         -5.07833898e-01,  -2.52877498e+00,  -3.10027575e+00,\n",
       "         -3.16426206e+00,  -2.75525767e-02,  -9.08142948e+00,\n",
       "         -3.49198437e+00,  -7.64666021e-01,  -8.81662941e+00,\n",
       "         -5.76153088e+00,  -7.92342424e+00,  -3.00987768e+00,\n",
       "         -3.99649167e+00,  -4.43154144e+00,  -1.14873905e+01,\n",
       "         -3.34710050e+00,  -7.03338194e+00,  -1.07595420e+00,\n",
       "         -9.31732170e-03,  -8.85389709e+00,  -8.23503304e+00,\n",
       "         -5.73243809e+00,  -3.09585500e+00,  -7.30990982e+00,\n",
       "         -8.96162415e+00,  -6.74901628e+00,  -7.30081940e+00,\n",
       "         -3.87950373e+00,  -2.41132855e+00,  -8.01904774e+00,\n",
       "         -1.48389554e+00,  -3.09095216e+00,  -4.00711834e-01,\n",
       "         -1.04508793e+00,  -4.29516840e+00,  -4.06943232e-01,\n",
       "         -1.02146759e+01,  -1.10012379e+01,  -4.69298649e+00,\n",
       "         -1.15925589e+01,  -6.90844133e-02,  -7.38132381e+00,\n",
       "         -3.10799003e+00,  -1.26729620e+00,  -8.76004410e+00,\n",
       "         -6.98567343e+00,  -8.90620041e+00,  -7.61675310e+00,\n",
       "         -2.47557020e+00,  -2.56837392e+00,  -4.15422678e+00,\n",
       "         -1.18032551e+00,  -7.30593109e+00,  -5.71943617e+00,\n",
       "         -1.05586205e+01,  -6.47064018e+00,  -7.52824879e+00,\n",
       "         -6.15547466e+00,  -5.41308117e+00,  -7.09290648e+00,\n",
       "         -2.77552915e+00,  -7.76052058e-01,  -9.44517994e+00,\n",
       "         -3.00828218e+00,  -4.02175426e-01,  -9.11198902e+00,\n",
       "         -7.40366459e-01,  -7.59360075e-01,  -8.07277775e+00,\n",
       "         -5.90825891e+00,  -7.74029684e+00,  -5.01015234e+00,\n",
       "         -1.67663205e+00,  -9.86841393e+00,  -1.09731722e+01,\n",
       "         -8.03800774e+00,  -9.41464043e+00,  -9.88289547e+00,\n",
       "         -1.03950386e+01,  -7.39561462e+00,  -1.06442595e+01,\n",
       "         -8.66086483e+00,  -1.18180287e+00,  -1.28541303e+00,\n",
       "         -1.28634536e+00,  -3.32834244e+00,  -4.70538425e+00,\n",
       "         -1.03668966e+01], dtype=float32)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run([model.q_density])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
