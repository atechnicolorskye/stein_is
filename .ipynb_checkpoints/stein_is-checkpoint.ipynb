{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.distributions as ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GMM(object):\n",
    "    def __init__(self, mu, sigma, weights, dim):\n",
    "        # Required parameters \n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.weights = weights\n",
    "        self.dim = dim\n",
    "        \n",
    "    def log_px(self, x):\n",
    "        # log_px = log(sum(exp(log(w_i) + log(p_i(x)))))\n",
    "        log_px = []\n",
    "        for i in range(weights.shape[0]):\n",
    "            mu_, sigma_ = self.mu[i] * tf.ones(dim), self.sigma[i] * tf.ones(dim)\n",
    "            mvn = ds.MultivariateNormalDiag(loc=mu_, scale_diag=sigma_)\n",
    "            # Calculate log_px for each component\n",
    "            log_px_i = tf.reduce_logsumexp(mvn.log_prob(x)) + tf.log(tf.to_float(weights[i]))\n",
    "            log_px.append(log_px_i)\n",
    "        return tf.reduce_logsumexp(log_px)\n",
    "    \n",
    "    def d_log_px(self, x):\n",
    "        # d_log_px = 1 / exp(log(sum(exp(log(w_i) + log(p_i(x)))))) \n",
    "        #            * sum(exp(log(w_i) + log(p_i(x)) + log(-(x - mu)/sigma^2)))\n",
    "        # Use symbolic differentiation instead\n",
    "        log_px = self.log_px(x)\n",
    "        return tf.gradients(log_px, [x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mu = np.array([1., -1.]); sigma = np.sqrt(np.array([0.1, 0.05])); weights = np.array([1./3, 2./3]); dim=6\n",
    "gmm = GMM(mu, sigma, weights, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class stein_is(object):\n",
    "    def __init__(self, gmm_model, mu, sigma, dim, n_leaders, n_followers, step_size=0.01): # n_trials, step_size=0.01):\n",
    "        # Required parameters\n",
    "        self.gmm_model = gmm_model\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.dim = dim\n",
    "        self.n_leaders = n_leaders\n",
    "        self.n_followers = n_followers\n",
    "        # self.n_trials = n_trials\n",
    "        self.step_size = step_size\n",
    "        self.eps = 1e-10\n",
    "        \n",
    "        # Set seed\n",
    "        seed = 30\n",
    "        \n",
    "        # Intialise\n",
    "        self.B, self.B_density, self.A = self.initialise_variables()\n",
    "        \n",
    "        # Register functions for debugging\n",
    "        # self.k_A_A, self.sum_grad_A_k_A_A, self.A_Squared, self.h_0 = self.construct_map()\n",
    "        # self.k_A_B, self.sum_grad_A_k_A_B, self.grad_A_grad_B_k_A_B, self.grad_B_k_A_B = self.apply_map()        \n",
    "        self.A, self.B, self.phi_B, self.grad_B_phi_B = self.svgd_update()\n",
    "        self.f_density = self.density_update()\n",
    "        \n",
    "    def initialise_variables(self):\n",
    "        init_distribution = tf.contrib.distributions.MultivariateNormalDiag(self.mu * tf.ones(dim), self.sigma * tf.ones(dim))\n",
    "        \n",
    "        # followers = tf.reshape(init_distribution.sample(self.n_trials * self.n_followers, seed=123), [self.n_trials, self.n_followers, self.h_dim] \n",
    "        # leaders = tf.reshape(init_distribution.sample(self.n_trials * self.n_leaders, seed=123), [self.n_trials, self.n_leaders, self.h_dim] \n",
    "        \n",
    "        followers = tf.reshape(init_distribution.sample(self.n_followers, seed=123), [self.n_followers, self.dim]) \n",
    "        f_density = init_distribution.log_prob(followers)\n",
    "        leaders = tf.reshape(init_distribution.sample(self.n_leaders, seed=123), [self.n_leaders, self.dim])\n",
    "                           \n",
    "        return followers, f_density, leaders\n",
    "                             \n",
    "    def construct_map(self):\n",
    "        # Calculate ||leader - leader'||^2/h_0, refer to leader as A as in SteinIS\n",
    "        x2_A_A_T = tf.multiply(2., tf.matmul(self.A, tf.transpose(self.A)))\n",
    "        A_Squared = tf.reduce_sum(tf.square(self.A), 1)\n",
    "        A_Distance = tf.add(tf.subtract(A_Squared, x2_A_A_T), tf.transpose(A_Squared))   \n",
    "        # h_0 = tf.divide(tf.add(median(A_Distance), self.eps), 2. * (tf.log(tf.cast(self.n_leaders, tf.float32)) + 1.))\n",
    "        h_0 = tf.divide(median(A_Distance), 2. * (tf.log(tf.to_float(self.n_leaders)) + 1.))\n",
    "        k_A_A = tf.exp(-tf.div(A_Distance, tf.square(h_0)))\n",
    "        sum_grad_A_k_A_A = tf.reduce_sum(tf.gradients(k_A_A, [self.A]), 1)\n",
    "        return k_A_A, sum_grad_A_k_A_A, A_Squared, h_0\n",
    "    \n",
    "    def apply_map(self):\n",
    "        # Calculate ||leader - follower||^2/h_0, refer to follower as B as in SteinIS\n",
    "        x2_A_B_T = tf.multiply(2., tf.matmul(self.A, tf.transpose(self.B)))\n",
    "        B_Squared = tf.reduce_sum(tf.square(self.B), 1)\n",
    "        A_B_Distance  = tf.add(tf.subtract(self.A_Squared, x2_A_B_T), B_Squared)\n",
    "        k_A_B = tf.exp(-tf.div(A_B_Distance, tf.square(self.h_0)))\n",
    "        sum_grad_A_k_A_B = tf.reduce_sum(tf.gradients(k_A_B, [self.A]), 1)\n",
    "        return k_A_B, sum_grad_A_k_A_B \n",
    "                    \n",
    "    def svgd_update(self):\n",
    "        self.k_A_A, self.sum_grad_A_k_A_A, self.A_Squared, self.h_0 = self.construct_map()\n",
    "        self.k_A_B, self.sum_grad_A_k_A_B = self.apply_map()\n",
    "        self.d_log_pA = self.gmm_model.d_log_px(self.A)[0]\n",
    "        sum_d_log_pA_T_k_A_A = tf.reduce_sum(tf.matmul(self.k_A_A, self.d_log_pA), 0)       \n",
    "        phi_A = (1. / tf.to_float(self.n_leaders)) * tf.add(sum_d_log_pA_T_k_A_A, self.sum_grad_A_k_A_A)\n",
    "        A = tf.add(self.A, self.step_size * phi_A)  \n",
    "        sum_d_log_pA_T_k_A_B = tf.reduce_sum(tf.matmul(self.k_A_B, self.d_log_pA), 0)       \n",
    "        phi_B = (1. / tf.to_float(self.n_leaders)) * tf.add(sum_d_log_pA_T_k_A_B, self.sum_grad_A_k_A_B)\n",
    "        B = tf.add(self.B, self.step_size * phi_B) \n",
    "        grad_B_phi_B = tf.gradients(phi_B, [self.B])\n",
    "        return A, B, phi_B, grad_B_phi_B[0]\n",
    "    \n",
    "    def density_update(self):\n",
    "        I = tf.eye(self.dim)\n",
    "        inv_abs_det_I_grad_B_phi = tf.map_fn(lambda x: 1./tf.abs(tf.matrix_determinant(tf.add(I, x))), self.grad_B_phi_B)\n",
    "        return tf.multiply(self.B_density, inv_abs_det_I_grad_B_phi) \n",
    "\n",
    "def median(x):\n",
    "    x = tf.reshape(x, [-1])\n",
    "    med = tf.floordiv(tf.shape(x)[0], 2)\n",
    "    check_parity = tf.equal(tf.to_float(med), tf.divide(tf.to_float(tf.shape(x)[0]), 2.))\n",
    "    def is_true():\n",
    "        return tf.reduce_sum(tf.nn.top_k(x, med+1).values[-2:]) / 2.\n",
    "    def is_false():\n",
    "        return tf.nn.top_k(x, med+1).values[-1]\n",
    "    return tf.cond(check_parity, is_true, is_false)                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 0.; sigma = 1.; dim = 6; n_leaders = 100; n_followers = 100;\n",
    "model = stein_is(gmm,  mu, sigma, dim, n_leaders, n_followers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = sess.run([model.f_density])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-17.76770592, -23.33740044, -15.72759342, -16.30294609,\n",
       "        -13.1185236 , -13.15271568, -13.47410679, -14.46366501,\n",
       "        -13.57182693, -19.8328495 , -12.7520771 , -11.94937229,\n",
       "        -14.21317768, -12.80694866, -13.18456078, -13.71470928,\n",
       "        -16.79034996, -16.43029976, -13.20270729, -13.75132656,\n",
       "        -16.23462677, -14.36654472, -14.99203396, -14.96727467,\n",
       "        -13.15204334, -14.03826427, -19.14477158, -12.66138744,\n",
       "        -17.09630013, -15.19779301, -12.29283905, -16.01700974,\n",
       "        -15.32056236, -14.30460453, -12.96601009, -15.25072002,\n",
       "        -16.40987778, -14.20990944, -14.57644463, -13.33748341,\n",
       "        -13.48271179, -15.38956642, -13.07788658, -13.06289196,\n",
       "        -11.55906105, -12.10474968, -13.22586155, -10.84550762,\n",
       "        -17.64623642, -18.55130577, -13.9176836 , -19.24054146,\n",
       "        -14.15886688, -14.41825676, -12.62604523, -15.58252048,\n",
       "        -16.09742546, -14.74546242, -16.07368279, -15.0657959 ,\n",
       "        -13.68206501, -12.66249752, -14.10882664, -11.79558945,\n",
       "        -16.08886147, -13.55189323, -18.04389381, -14.45329571,\n",
       "        -14.81221676, -14.37201023, -14.23504162, -14.28787899,\n",
       "        -13.96692562, -11.85381603, -16.84576607, -13.15099812,\n",
       "        -28.80789185, -16.4809494 , -12.2590189 , -11.96818447,\n",
       "        -15.15344906, -14.12467861, -16.39096069, -14.10642529,\n",
       "        -12.35852146, -17.20976639, -18.51922798, -15.2695961 ,\n",
       "        -17.05432892, -17.25281715, -17.84143448, -14.83737564,\n",
       "        -18.15939903, -16.29272461, -12.74602509, -12.40869617,\n",
       "        -14.13204575, -13.36123371, -13.55464745, -17.83107185], dtype=float32)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
